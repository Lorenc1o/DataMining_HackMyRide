{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7QedDt3LWRH"
      },
      "source": [
        "# Retrieving Arrival Time from Real Time Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Environment\n",
        "\n",
        "In this part, we will set up the environment.\n",
        "- Installing google cloud environtment\n",
        "- Installing necessary libraries\n",
        "- Importing necessary libraries\n",
        "- Define shared variables and functions\n",
        "\n",
        "<br/>\n",
        "\n",
        "### Saving file via VM environment\n",
        "**Important notes: We use gcsfuse to combine google cloud storage and google collab directories, but there's permission issue when saving a file in vm instance**\n",
        "\n",
        "- Upload credential file to content directory in google collab\n",
        "- enable VM environment settings\n",
        "- replace this syntax<br />From this:\n",
        "```\n",
        "   to_csv('content/main/path')\n",
        "```   \n",
        "To this:\n",
        "```\n",
        "  to_csv(f'gs://{bucket.name}/path')\n",
        "```\n"
      ],
      "metadata": {
        "id": "4mhewZucqxQo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8jF-sA3ENEx",
        "outputId": "6222179c-177f-423f-87fc-b3084a1513d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2426  100  2426    0     0  65567      0 --:--:-- --:--:-- --:--:-- 65567\n",
            "OK\n",
            "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
            "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is no longer signed.\n",
            "gcsfuse is already the newest version (0.41.9).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 315 not upgraded.\n",
            "mkdir: cannot create directory ‘main’: File exists\n",
            "2022/12/18 16:11:46.840036 Start gcsfuse/0.41.9 (Go version go1.18.4) for app \"\" using mount point: /content/main\n",
            "2022/12/18 16:11:46.859359 Opening GCS connection...\n",
            "2022/12/18 16:11:47.901115 Mounting file system \"datamining-ulb\"...\n",
            "2022/12/18 16:11:47.904848 File system has been successfully mounted.\n",
            "arrival_time\tLines_vehiclePositions\tordered_lines  schedule\n",
            "departure_time\tnew_departure_time\tpunctuality    vehiclePositions\n",
            "esri_files\tnormalized_lines\tregularity\n",
            "Requirement already satisfied: jenkspy in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jenkspy) (1.21.5)\n",
            "Requirement already satisfied: ckwrap in /usr/local/lib/python3.7/dist-packages (0.1.10)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from ckwrap) (0.29.28)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ckwrap) (1.21.5)\n",
            "Requirement already satisfied: dbscan1d in /usr/local/lib/python3.7/dist-packages (0.1.6)\n",
            "Requirement already satisfied: flake8 in /usr/local/lib/python3.7/dist-packages (from dbscan1d) (5.0.4)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.7/dist-packages (from dbscan1d) (22.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dbscan1d) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black->dbscan1d) (3.10.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black->dbscan1d) (0.4.3)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black->dbscan1d) (2.0.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from black->dbscan1d) (8.1.3)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from black->dbscan1d) (0.10.3)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.7/dist-packages (from black->dbscan1d) (2.6.0)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black->dbscan1d) (1.5.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0.0->black->dbscan1d) (4.2.0)\n",
            "Requirement already satisfied: pycodestyle<2.10.0,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from flake8->dbscan1d) (2.9.1)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from flake8->dbscan1d) (0.7.0)\n",
            "Requirement already satisfied: pyflakes<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from flake8->dbscan1d) (2.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0.0->black->dbscan1d) (3.7.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (2022.11.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.7/dist-packages (2022.11.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.18.1)\n",
            "Requirement already satisfied: fsspec==2022.11.0 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2022.11.0)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (3.8.3)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs) (0.4.6)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (3.10.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (0.4.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (21.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.55.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.0.7)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "  Initiate Auth to access google cloud storage & setup necessary package\n",
        "'''\n",
        "\n",
        "# Disable in vm, enable in colab local\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "# Disable in colab environment\n",
        "from google.cloud import storage\n",
        "client = storage.Client.from_service_account_json(\"/content/fresh-tape-370916-d7d7c2e7c5c4.json\")\n",
        "bucket = client.get_bucket('datamining-ulb')\n",
        "\n",
        "# Enable in collab runtime\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "!mkdir main\n",
        "!gcsfuse --implicit-dirs datamining-ulb main\n",
        "\n",
        "!ls main\n",
        "\n",
        "!pip install jenkspy\n",
        "!pip install ckwrap\n",
        "!pip install dbscan1d\n",
        "!pip install fsspec\n",
        "!pip install gcsfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdDfGg2ZGQp-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  Import common library\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime\n",
        "import decimal\n",
        "import os\n",
        "\n",
        "from glob import glob\n",
        "from dateutil import parser\n",
        "import csv\n",
        "from io import StringIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pQs_LKBUH9f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        " Helper functions & constants\n",
        "'''\n",
        "\n",
        "decimal.getcontext().prec = 3\n",
        "TO_MS = decimal.Decimal(\"1000\")\n",
        "TWO_HOURS = 7200000\n",
        "STOP_DISTANCE_THRESHOLD = 5 # In metres\n",
        "AVERAGE_SPEED = {\"bus\": decimal.Decimal(\"4.305\"), \"tram\": decimal.Decimal(\"4.5\"), \"metro\": decimal.Decimal(\"7.75\")}  # in metre/second\n",
        "\n",
        "'''\n",
        "Convert epoch milisecond to datetime format:\n",
        " i.e 165278383333 -> 2022-12-01 12:23:20\n",
        "'''\n",
        "def epoch_ms_to_datetime(epoch_ms: int) -> datetime.datetime:\n",
        "  return datetime.datetime.fromtimestamp(epoch_ms / int(TO_MS))\n",
        "\n",
        "'''\n",
        "  Convert datetime to epoch:\n",
        "  i.e 2022-12-01 12:23:20 -> 165278383333\n",
        "'''\n",
        "def datetime_to_epoch(datetime: datetime) -> int:\n",
        "  return datetime.timestamp()\n",
        "\n",
        "def save_csv(path, df):\n",
        "  \"\"\"for saving csv files\"\"\"\n",
        "  save_df = df.to_csv(sep=\",\", index=False, quotechar='\"', quoting=csv.QUOTE_ALL, encoding=\"UTF-8\")\n",
        "  blob = bucket.blob(path)\n",
        "  blob.upload_from_string(data=save_df)\n",
        "\n",
        "def save_np_csv(path,np_arr):\n",
        "  \"\"\"for saving numpy arrays to csv\"\"\"\n",
        "  s = StringIO()\n",
        "  save_np = np.savetxt(s, np_arr, delimiter=\",\", fmt='%s')\n",
        "  blob = bucket.blob(path)\n",
        "  blob.upload_from_string(data=s.getvalue())\n",
        "\n",
        "def get_column_idx(column_label, df):\n",
        "  return [idx for idx, col in enumerate(df.columns) if col==column_label][0]\n",
        "\n",
        "def get_real_date(year, month, day, hour, minute, second):\n",
        "  date = datetime.datetime(year,month,day)\n",
        "  if hour > 23:\n",
        "    date += datetime.timedelta(days=1)\n",
        "    hour = hour % 24\n",
        "  date += datetime.timedelta(hours=hour, minutes=minute, seconds=second)\n",
        "  return date"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning The Data\n",
        "\n",
        "We follow several steps to clean the data:\n",
        "\n",
        "## Marking Technical Stops\n",
        "\n",
        "- We extract technical stops from the shapefile data. We loaded the data using postgis and export it to csv files to retrieve it in python\n",
        "- After that put the orders of the stops as new parameter by matching the pointID and directionId from the ac. The orders is only used to verify the stops and helps us to visualize the data.\n",
        "- We also put mark into the stops if we cannot find the stops id or the direction id from the actual stops data. We will mark this pointID as techincal stop\n",
        "\n",
        "## Segments Data\n",
        "\n",
        "- We will separated the data by its type (bus, tram, metro) and also identify whether the line has abnormal distance (contains only 0 and 1) or has normal distance.\n",
        "\n",
        "## Distance Normalization\n",
        "\n",
        "### Abnormal Distance\n",
        "After identfying the abnormal and normal distance, we will normalize the distance using this algorithm:\n",
        "\n",
        "1) For each timestamp, each direction, each pointID, count how many 1s\n",
        "2) say there are N, then we get distancr from pointID to the next one in the Sequence, csll this distance D\n",
        "3) divide D/(N+1)=p\n",
        "4) set distances to be p, 2p, ..., Np\n",
        "\n",
        "### Normalize Non-Zero Distance\n",
        "We will convert non-zero distance, and calculate estimated timestamp for -5 & +5 distanceFromPoint.\n",
        "\n",
        "1. Pick non zero distance and interpolate it by doing estimated timestamp calculation for -5 & +5 distance.\n",
        "2. We will then sort the data based on the estimated timestamp\n",
        "3. If we find consecutive non-zero data with the same distance, we will remove it to declutter the data.\n",
        "\n",
        "### Grouping the Data\n",
        "We will group the result by line id and point id and separate it into different files. It will make the clustering and analyzing result much more easier since it is already grouped per stop id.\n"
      ],
      "metadata": {
        "id": "LfwgOQNEuCU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2plV2dAvGjIZ",
        "outputId": "56515dfd-55e0-49f0-81f7-28abab662fc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Line71', 'Line93', 'Line20', 'Line78', 'Line6', 'Line28', 'Line95', 'Line33', 'Line27', 'Line38', 'Line47', 'Line79', 'Line13', 'Line62', 'Line48', 'Line43', 'Line58', 'Line92', 'Line46', 'Line12', 'Line42', 'Line57', 'Line74', 'Line1', 'Line88', 'Line89', 'Line77', 'Line54', 'Line19', 'Line39', 'Line17', 'Line56', 'Line69', 'Line72', 'Line81', 'Line61', 'Line8', 'Line80', 'Line36', 'Line50', 'Line76', 'Line5', 'Line7', 'Line70', 'Line9', 'Line21', 'Line60', 'Line63', 'Line64', 'Line75', 'Line14', 'Line98', 'Line41', 'Line45', 'Line66', 'Line53', 'Line59', 'Line51', 'Line55', 'Line82', 'Line44', 'Line37', 'Line2', 'Line29', 'Line34', 'Line49', 'Line86', 'Line25', 'Line65', 'Line4', 'Line97', 'Line3', 'Line87', 'Line83'}\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "  Filter out technical stops or non existing stops from the real data\n",
        "'''\n",
        "\n",
        "# Extract actual stops\n",
        "actual_df = pd.read_csv(\"/content/main/data/actual_stops.csv\")\n",
        "size = actual_df.shape[0]\n",
        "actual_df[\"stop_id\"] = [re.sub(\"\\D\", \"\", x.strip()) for x in actual_df[\"stop_id\"]]\n",
        "\n",
        "actual_df[\"succession\"] = actual_df[\"succession\"].astype(str)\n",
        "actual_df[\"numero_lig\"] = actual_df[\"numero_lig\"].astype(str)\n",
        "actual_df[\"variante\"] = actual_df[\"variante\"].astype(str)\n",
        "actual_stops = set(actual_df[\"stop_id\"])\n",
        "\n",
        "# Extract Line stop sequence\n",
        "extracted_lines = {}\n",
        "\n",
        "delimiters = np.array([\"_\" for _ in range(size)])\n",
        "stops_info = (actual_df[\"numero_lig\"].to_numpy() + delimiters +  actual_df[\"succession\"].to_numpy() + delimiters + actual_df[\"variante\"].to_numpy() + delimiters + actual_df[\"stop_id\"].to_numpy()).tolist()\n",
        "\n",
        "prev_succession = -999999\n",
        "direction = \"\"\n",
        "for stops in stops_info[::-1]:\n",
        "    info = stops.split(\"_\")\n",
        "    if int(prev_succession) < int(info[1]):\n",
        "        direction = info[3]\n",
        "\n",
        "    data = extracted_lines.setdefault(info[0], {})\n",
        "\n",
        "    succession_info = data.setdefault(info[3], {})\n",
        "    succession_info[direction] = info[1]\n",
        "\n",
        "    data[info[3]] = succession_info\n",
        "    extracted_lines[info[0]] = data\n",
        "\n",
        "    line_direction = data.setdefault(\"direction\", {})\n",
        "    visited_stops = line_direction.setdefault(direction, set())\n",
        "    visited_stops.add(info[3])\n",
        "\n",
        "    line_direction[direction] = visited_stops\n",
        "    data[\"direction\"] = line_direction\n",
        "\n",
        "    prev_succession = info[1]\n",
        "\n",
        "'''\n",
        "Pair stops with the succession line,\n",
        " 1. if there's multiple sucession line check whether direction is the same\n",
        " 2. If not, try to find direction id in the direction stops\n",
        " 3. otherwise set sucession to 99999\n",
        "'''\n",
        "\n",
        "lines = []\n",
        "names = set()\n",
        "for file in glob(\"/content/main/ordered_lines/*.csv\"):\n",
        "  name = file.strip().split(\"/\")[4].split(\"_\")[0]\n",
        "  names.add(name)\n",
        "\n",
        "print(names)\n",
        "for file in glob(\"/content/main/Lines_vehiclePositions/*.csv\"):\n",
        "    if \"null\" in file or file in names:\n",
        "      continue\n",
        "\n",
        "    df = pd.read_csv(file)\n",
        "    df[\"lineID\"] = df[\"lineID\"].astype(str)\n",
        "    df[\"directionID\"] = df[\"directionID\"].astype(str)\n",
        "    df[\"pointID\"] = df[\"pointID\"].astype(str).apply(lambda x: x.strip().split(\" \")[0])\n",
        "\n",
        "    delimiters = np.array([\"_\" for _ in range(df.shape[0])])\n",
        "    stops_infos = (df[\"lineID\"].to_numpy() + delimiters + df[\"directionID\"].to_numpy() + delimiters + df[\"pointID\"].to_numpy() + delimiters + df.index.astype(str).values).tolist()\n",
        "\n",
        "    succession_arr = []\n",
        "    for stop in stops_infos:\n",
        "        info = stop.split(\"_\")\n",
        "        data = extracted_lines[info[0]]\n",
        "\n",
        "        succession = 999999\n",
        "        if info[2] not in data:\n",
        "            succession_arr.append(succession)\n",
        "            continue\n",
        "\n",
        "        succession_info = data[info[2]]\n",
        "        if info[1] not in succession_info:\n",
        "            succession_arr.append(succession)\n",
        "            continue\n",
        "\n",
        "        succession = succession_info[info[1]]\n",
        "        succession_arr.append(succession)\n",
        "\n",
        "    df[\"succession\"] = np.array(succession_arr)\n",
        "\n",
        "    df[\"lineID\"] = df[\"lineID\"].astype(int)\n",
        "    df[\"directionID\"] = df[\"directionID\"].astype(int)\n",
        "    df[\"pointID\"] = df[\"pointID\"].astype(int)\n",
        "    df[\"succession\"] = df[\"succession\"].astype(int)\n",
        "\n",
        "    name = file.strip().split(\"/\")[4].split(\"_\")[0]\n",
        "    df.sort_values([\"directionID\", \"time\",  \"succession\"], ascending=[True, True, True]).to_csv(f\"/content/main/ordered_lines/{name}_ordered.csv\", index=False)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKiFHnfNSQ0J",
        "outputId": "99232e56-c219-4604-be5f-bb998211e1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L25: 127286\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Marking technical stops\n",
        "'''\n",
        "# Extract all stops\n",
        "# using stops.txt as it has all the stops\n",
        "actual_df = pd.read_csv(\"/content/main/schedule/gtfs23Sept/stops.csv\")\n",
        "actual_df[\"stop_id\"] = [re.sub(\"\\D\", \"\", x.strip()) for x in actual_df[\"stop_id\"]]\n",
        "actual_stops = set(actual_df[\"stop_id\"])\n",
        "\n",
        "for filename in glob(\"/content/main/ordered_lines/*.csv\"):\n",
        "# filename = \"/content/main/ordered_lines/Line25_ordered.csv\"\n",
        "  if \"null\" in filename:\n",
        "    continue\n",
        "\n",
        "  cur_lineID = str(int(filename.split('/')[-1].split('_')[0][4:]))\n",
        "  df = pd.read_csv(filename)\n",
        "  df[\"pointID\"] = df[\"pointID\"].astype(str).apply(lambda x: x.strip().split(\" \")[0])\n",
        "\n",
        "  df['technical'] = False\n",
        "  for idx, row in df.iterrows():\n",
        "    if (str(row['pointID']) not in actual_stops) or (str(row['directionID']) not in actual_stops):\n",
        "      df.at[idx, 'technical'] = True\n",
        "\n",
        "  df.sort_values([\"directionID\", \"time\",  \"succession\",], ascending=[True, True, True]).to_csv(f\"/content/main/ordered_lines/Line{cur_lineID}_ordered.csv\", index=False)\n",
        "  print(f\"L{cur_lineID}: {df.loc[df['technical'] == True].shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx8-do4wkpQ5"
      },
      "outputs": [],
      "source": [
        "# removing trailing 0s from stop_ids\n",
        "actual_filename = \"/content/main/data/actual_stops.csv\"\n",
        "actual_df = pd.read_csv(actual_filename)\n",
        "# actual_df[\"stop_id\"] = [re.sub(\"\\D\", \"\", x.strip()) for x in actual_df[\"stop_id\"]]\n",
        "actual_df[\"stop_id\"] = actual_df[\"stop_id\"].apply(lambda x: x.lstrip(\"0\"))\n",
        "actual_df.to_csv(actual_filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXNn-gxrUaNw"
      },
      "outputs": [],
      "source": [
        "# removing unnecessary indeces\n",
        "for filename in glob(\"/content/main/ordered_lines/*.csv\"):\n",
        "  if \"null\" in filename:\n",
        "    continue\n",
        "\n",
        "  df = pd.read_csv(filename)\n",
        "  cur_lineID = str(int(filename.split('/')[-1].split('_')[0][4:]))\n",
        "\n",
        "  for column in df.columns:\n",
        "    if 'Unnamed' in column:\n",
        "      df = df.drop([column], axis=1)\n",
        "\n",
        "  df.sort_values([\"directionID\", \"time\",  \"succession\",], ascending=[True, True, True]).to_csv(f\"/content/main/ordered_lines/Line{cur_lineID}_ordered.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnb68J2-sr-d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Analyze which is the tram, bus or metro. Also, analyze whether the stops is only consist of 0 or 1 distance\n",
        "'''\n",
        "# Manually set the data from bus, tram, or metro based on stib website\n",
        "metro = {\"1\", \"2\", \"5\", \"6\"}\n",
        "tram = {\"3\", \"4\", \"7\", \"8\", \"9\", \"19\", \"25\", \"39\", \"44\", \"51\", \"55\", \"62\", \"81\", \"82\", \"92\", \"93\", \"97\"}\n",
        "\n",
        "# Iterate to stops directory\n",
        "for filename in glob(\"/content/main/ordered_lines/*.csv\"):\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    line = str(df[\"lineID\"][0])\n",
        "    line_type =  \"metro\" if line in metro else \"tram\" if line in tram  else \"bus\"\n",
        "    is_zero_one = all(distance == 0 or distance == 1 for distance in df[\"distancefromPoint\"].astype(int).to_numpy())\n",
        "    folder_name = {True: \"AbnormalDistance\", False: \"NormalDistance\"}\n",
        "\n",
        "    name = filename.strip().split(\"/\")[4].split(\"_\")[0]\n",
        "    df.to_csv(f\"/content/main/ordered_lines/{folder_name[is_zero_one]}/{line_type}/{name}_ordered.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF2gLXjJM6-B"
      },
      "outputs": [],
      "source": [
        "# Analyzing metro (0/1) lines\n",
        "filename = \"/content/main/ordered_lines/Line1_ordered.csv\"\n",
        "df = pd.read_csv(filename)\n",
        "df = df.loc[df['technical']==False]\n",
        "df[\"lineID\"] = df[\"lineID\"].astype(str)\n",
        "df[\"directionID\"] = df[\"directionID\"].astype(str)\n",
        "df[\"pointID\"] = df[\"pointID\"].astype(str).apply(lambda x: x.strip().split(\" \")[0])\n",
        "cur_lineID = str(int(filename.split('/')[-1].split('_')[0][4:]))\n",
        "\n",
        "ones_df = df.loc[df['distancefromPoint']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzoGJS-frEhZ"
      },
      "outputs": [],
      "source": [
        "filename = \"/content/main/data/distance-between-points.csv\"\n",
        "distances_df = pd.read_csv(filename)\n",
        "\n",
        "# detailed but does not contain all stops\n",
        "actual_df = pd.read_csv(\"/content/main/data/actual_stops.csv\")\n",
        "actual_df[\"succession\"] = actual_df[\"succession\"].astype(str)\n",
        "actual_df[\"numero_lig\"] = actual_df[\"numero_lig\"].astype(str)\n",
        "actual_df[\"variante\"] = actual_df[\"variante\"].astype(str)\n",
        "actual_df[\"stop_id\"] = [re.sub(\"\\D\", \"\", x.strip()) for x in actual_df[\"stop_id\"]]\n",
        "\n",
        "# contains all stops but not as detailed\n",
        "stops_df = pd.read_csv(\"/content/main/schedule/gtfs23Sept/stops.csv\")\n",
        "stops_df[\"stop_id\"] = [re.sub(\"\\D\", \"\", x.strip()) for x in stops_df[\"stop_id\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBJdUkq3tWzR"
      },
      "outputs": [],
      "source": [
        "def get_next_stop(cur_pointID, cur_dirID, cur_lineID):\n",
        "  '''\n",
        "  gets decriptons of current and next bus stops,\n",
        "  takes into consideration the directionID (cur_dirID), lineID (cur_lineID) and this stop's pointID (cur_pointID)\n",
        "  accounts for cases when dirID is not the terminal, but an earlier stop\n",
        "\n",
        "  requires stops.csv set as stops_df, actual_stops.csv as actual_df\n",
        "  '''\n",
        "  dirID_info = stops_df.loc[stops_df['stop_id'] == cur_dirID]\n",
        "  if dirID_info.shape[0] == 0:\n",
        "    print(f'No such stop: {cur_dirID}')\n",
        "  dirID_desc = dirID_info.iloc[0]['stop_name']\n",
        "\n",
        "  pointID_info = stops_df.loc[stops_df['stop_id'] == cur_pointID]\n",
        "  if pointID_info.shape[0] == 0:\n",
        "    print(f'No such stop: {cur_pointID}')\n",
        "  pointID_desc = pointID_info.iloc[0]['stop_name']\n",
        "\n",
        "  cur_pointID_info = actual_df.loc[(actual_df['descr_fr']==pointID_desc) & (actual_df['numero_lig'] == cur_lineID) & (actual_df['terminus']==dirID_desc)]\n",
        "  if cur_pointID_info.shape[0] == 0:\n",
        "    # if the directionId is not the actual terminal but an earlier stop\n",
        "    # find the actual terminal from the \"direcitionID\" information\n",
        "    cur_dirID_info = actual_df.loc[(actual_df['descr_fr']==dirID_desc) & (actual_df['numero_lig'] == cur_lineID) & (actual_df['stop_id']==cur_dirID)]\n",
        "    if cur_dirID_info.shape[0] != 1:\n",
        "      print(f'Not 1 match for cur_dirID {cur_dirID}, descr {dirID_desc}')\n",
        "\n",
        "      # todo: if the stop_id of \"directionID\" is not of this line, maybe the description of the bus stop is correct\n",
        "      # find the combination of this line where current pointID succession is smaller than \"directionID\" succession\n",
        "\n",
        "    actual_terminal = cur_dirID_info.iloc[0]['terminus']\n",
        "\n",
        "    cur_pointID_info = actual_df.loc[(actual_df['descr_fr']==pointID_desc) & (actual_df['numero_lig'] == cur_lineID) & (actual_df['terminus']==actual_terminal)]\n",
        "\n",
        "  elif cur_pointID_info.shape[0] != 1:\n",
        "    print(f'Not 1 match for cur_pointID {pointID_desc} terminus {actual_terminal}')\n",
        "\n",
        "  else:\n",
        "    actual_terminal = dirID_desc\n",
        "\n",
        "  cur_succession = cur_pointID_info.iloc[0]['succession']\n",
        "\n",
        "  # find the pointId of the next succession\n",
        "  next_succession = str(int(cur_succession)+1)\n",
        "  next_pointID_info = actual_df.loc[(actual_df['numero_lig'] == cur_lineID) & (actual_df['terminus']==actual_terminal) & (actual_df['succession']==next_succession)]\n",
        "\n",
        "  if next_pointID_info.shape[0] != 1:\n",
        "    print(f'Not 1 match for next_pointID for terminus: {actual_terminal}, succession {succession}')\n",
        "\n",
        "  dirID_desc = dirID_info.iloc[0]['stop_name']\n",
        "\n",
        "  return pointID_desc, next_pointID_info.iloc[0]['descr_fr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BC_SAMWS6VN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Estimating distances for 0/1 lines (metros)\n",
        "'''\n",
        "for filename in glob(\"/content/main/ordered_lines/AbnormalDistance/*/*.csv\"):\n",
        "  cur_lineID = str(int(filename.split('/')[-1].split('_')[0][4:]))\n",
        "  print('line', cur_lineID)\n",
        "  df = pd.read_csv(filename)\n",
        "  df = df.loc[df['technical']==False]\n",
        "  df[\"lineID\"] = df[\"lineID\"].astype(str)\n",
        "  df[\"directionID\"] = df[\"directionID\"].astype(str)\n",
        "  df[\"pointID\"] = df[\"pointID\"].astype(str).apply(lambda x: x.strip().split(\" \")[0])\n",
        "\n",
        "  ones_df = df.loc[df['distancefromPoint']==1]\n",
        "\n",
        "  for cur_time in ones_df.time.unique():\n",
        "      # for each timestamp\n",
        "      cur_time_set = ones_df.loc[ones_df['time']==cur_time]\n",
        "\n",
        "      for cur_pointID in cur_time_set.pointID.unique():\n",
        "          # for each bus_stop in that timestamp\n",
        "          cur_pointID_set = cur_time_set.loc[cur_time_set['pointID']==cur_pointID]\n",
        "\n",
        "          for cur_dirID in cur_pointID_set.directionID.unique():\n",
        "              target_set = cur_pointID_set.loc[cur_pointID_set['directionID']==cur_dirID]\n",
        "              # print(cur_time, cur_pointID, cur_dirID)\n",
        "              N = len(target_set)\n",
        "              try:\n",
        "                  pointID_desc, next_pointID_descr = get_next_stop(cur_pointID, cur_dirID, cur_lineID)\n",
        "                  D = distances_df.loc[(distances_df['from_descr_fr']==pointID_desc) & (distances_df['to_descr_fr']==next_pointID_descr)].iloc[0]['dist']\n",
        "                  p = D/(N+1)\n",
        "                  i = 1\n",
        "                  for idx, row in target_set.iterrows():\n",
        "                      ones_df.at[idx,'distancefromPoint']=i*p\n",
        "                      i+=1\n",
        "              except Exception as e:\n",
        "                for idx, row in target_set.iterrows():\n",
        "                    ones_df.at[idx,'distancefromPoint']=999999\n",
        "\n",
        "  # saving to the actual df\n",
        "  df.loc[df.index.isin(ones_df.index), ['distancefromPoint']] = ones_df[['distancefromPoint']]\n",
        "  df.sort_values([\"directionID\", \"time\",  \"pointID\",], ascending=[True, True, True]).to_csv(f\"/content/main/ordered_lines/Line{cur_lineID}_1_ordered.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85KgMY27UR3A"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Normalize the distance\n",
        " The algorithm will work as follow:\n",
        " 1. We will change the distance for non-zero distance point by subtracting the time with how much time passed since the bus in the 0 distance of the point\n",
        " 2. We will use the bus/tram average speed to calculate the distance passed\n",
        "'''\n",
        "\n",
        "# Only process normal line\n",
        "for filename in glob(\"/content/main/ordered_lines/NormalDistance/tram/*.csv\"):\n",
        "    line_df = pd.read_csv(filename)\n",
        "    line_df = line_df[(line_df[\"technical\"] == False) & (line_df[\"distancefromPoint\"]) != 999999]\n",
        "    line_df = line_df.sort_values([\"directionID\", \"pointID\", \"time\"])\n",
        "\n",
        "    line_id = filename.split(\"/\")[-1].split(\"_\")[0]\n",
        "    line_type = filename.split(\"/\")[5].strip()\n",
        "\n",
        "    columns = [\"time\", \"lineID\", \"directionID\", \"distancefromPoint\", \"pointID\", \"succession\"]\n",
        "\n",
        "    size = line_df.shape[0]\n",
        "    delimiters = np.array([\"_\" for _ in range(size)])\n",
        "\n",
        "    line_infos = None\n",
        "    for idx, column in enumerate(columns):\n",
        "        line_df[column] = line_df[column].astype(str)\n",
        "        if line_infos is None:\n",
        "            line_infos = line_df[column].to_numpy() + delimiters\n",
        "        elif line_infos is not None and idx < len(columns)-1:\n",
        "            line_infos += line_df[column].to_numpy() + delimiters\n",
        "        else:\n",
        "            line_infos += line_df[column].to_numpy()\n",
        "\n",
        "\n",
        "    # Array for building new dataset\n",
        "    converted_timestamp = []\n",
        "    converted_distances = []\n",
        "    direction_ids = []\n",
        "    point_ids = []\n",
        "    line_ids = []\n",
        "    orders = []\n",
        "    original_timestamp = []\n",
        "    original_distances = []\n",
        "\n",
        "    for idx, line_info in enumerate(line_infos):\n",
        "        line = line_info.split(\"_\")\n",
        "\n",
        "        # Unpack information\n",
        "        timestamp = int(line[0])\n",
        "        line_id = line[1]\n",
        "        direction_id = line[2]\n",
        "        distance = decimal.Decimal(line[3])\n",
        "        point_id = line[4]\n",
        "        order = int(line[5])\n",
        "\n",
        "        # Ignore stopping bus\n",
        "        if int(distance) == 0:\n",
        "            line_ids.append(line_id)\n",
        "            direction_ids.append(direction_id)\n",
        "            point_ids.append(point_id)\n",
        "            orders.append(order)\n",
        "\n",
        "            original_timestamp.append(epoch_ms_to_datetime(timestamp))\n",
        "            converted_timestamp.append(epoch_ms_to_datetime(timestamp))\n",
        "            original_distances.append(distance)\n",
        "            converted_distances.append(distance)\n",
        "            continue\n",
        "\n",
        "        # Calculate -5 meters and +5 meters from stop\n",
        "        # The interpolation threshold is chosen by using heuristic method\n",
        "        # We will consider bus position -5 metres or +5 metres from stop is considered as stopping\n",
        "        # Normalized the timestamp\n",
        "        timestamp_plus = timestamp - (int(((distance+STOP_DISTANCE_THRESHOLD) / AVERAGE_SPEED[line_type])*TO_MS))\n",
        "        timestamp_min = timestamp - (int(((distance-STOP_DISTANCE_THRESHOLD) / AVERAGE_SPEED[line_type])*TO_MS))\n",
        "\n",
        "        timestamps = [timestamp_min, timestamp_plus]\n",
        "        time_passed = [(int((distance-STOP_DISTANCE_THRESHOLD) / AVERAGE_SPEED[line_type])), (int((distance+STOP_DISTANCE_THRESHOLD) / AVERAGE_SPEED[line_type]))]\n",
        "\n",
        "        # Add the interpolation data\n",
        "        for idx in range(len(timestamps)):\n",
        "            line_ids.append(line_id)\n",
        "            direction_ids.append(direction_id)\n",
        "            point_ids.append(point_id)\n",
        "            orders.append(order)\n",
        "\n",
        "            original_timestamp.append(epoch_ms_to_datetime(timestamp))\n",
        "            converted_timestamp.append(epoch_ms_to_datetime(timestamps[idx]))\n",
        "\n",
        "            original_distances.append(distance)\n",
        "            converted_distances.append(distance - (AVERAGE_SPEED[line_type]*time_passed[idx]))\n",
        "\n",
        "    columns = {\n",
        "                \"line_id\": np.array(line_ids),\n",
        "               \"direction_id\": np.array(direction_ids),\n",
        "               \"point_id\": np.array(point_ids),\n",
        "               \"order\": np.array(orders),\n",
        "               \"original_timestamp\": np.array(original_timestamp),\n",
        "               \"converted_timestamp\": np.array(converted_timestamp),\n",
        "               \"original_distance\": np.array(original_distances),\n",
        "               \"converted_distance\": np.array(converted_distances)\n",
        "              }\n",
        "\n",
        "\n",
        "    converted_df = pd.DataFrame(columns).sort_values([\"direction_id\", \"order\", \"converted_timestamp\"], ascending=[True, True, True]).reset_index(drop=True)\n",
        "\n",
        "    distances = converted_df['original_distance'].to_numpy()\n",
        "    points = converted_df['point_id'].to_numpy()\n",
        "    removed_idxs = []\n",
        "\n",
        "    # Remove consecutive distance\n",
        "    for idx in range(len(distances)):\n",
        "      if distances[idx] != 0 and idx < len(distances) - 2 and int(distances[idx]) == int(distances[idx+2]) and str(points[idx]) == str(points[idx+2]):\n",
        "        removed_idxs.append(idx+1)\n",
        "    converted_df.drop(index=removed_idxs)\n",
        "\n",
        "    # Group result\n",
        "    results = [group[1] for group in converted_df.groupby([converted_df.direction_id, converted_df.point_id])]\n",
        "    for group in results:\n",
        "      direction = group.iloc[0]['direction_id']\n",
        "      point_id = group.iloc[0]['point_id']\n",
        "\n",
        "      group.to_csv(f\"/content/main/normalized_lines/{line_id}_{direction}_{point_id}\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting The Departure Time\n",
        "\n",
        "We will cluster the normalized data based on the timestamp. This needs to be done in order to get estimated time when a bus/buses arrive at particular stop within the same time range. The time range need to be close enough to be considered as the same cluster. The departure time is calculated by picking the maximum timestamp from the same cluster.\n",
        "\n",
        "In this approach we will use 4 algorithm to try cluster the data into the same group. We will use:<br/>\n",
        "1) Ckmeans 1D DP<br/>\n",
        "2) Dbscan 1D<br/>\n",
        "3) Jenkins Natural Breaks<br/>\n",
        "4) Kernel Density Estimation\n",
        "\n",
        "We pick the algorithm based on how well the perform for one dimensional data, since most clustering algorithm is not optimized for handling such cases.\n",
        "\n",
        "Steps to get the cluster: <br/>\n",
        "1) Obtain the number of clusters needed. It is defined by the number of buses in pointID based on the schedule. The number of clusters is only needed for ckmeans and jenkins algorithm. <br/>\n",
        "2) Cluster the data using the algorithm and append the label to the dataset <br/>"
      ],
      "metadata": {
        "id": "caeust2JuhHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing schedules that are null\n",
        "path = '/content/main/arrival_time/data/schedules/realFinal/central_lines_schedule_puncTrunc.csv'\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "GFsUDQbHlCEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[((df['avg_headway']!='None') | (df['estimated_departure_time']!='None'))]"
      ],
      "metadata": {
        "id": "5LcYjDAqlHeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_csv('arrival_time/data/schedules/realFinal/central_lines_schedule_puncTrunc_NoNone.csv', df)"
      ],
      "metadata": {
        "id": "q4NUbsmFl6Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFlCv6duUw9f"
      },
      "outputs": [],
      "source": [
        "from numpy import array, linspace, sort\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from matplotlib.pyplot import plot\n",
        "from scipy.signal import argrelextrema\n",
        "from jenkspy import JenksNaturalBreaks\n",
        "from dbscan1d.core import DBSCAN1D\n",
        "import ckwrap\n",
        "\n",
        "# Extract all stops\n",
        "# using stops.txt as it has all the stops\n",
        "stops_df = pd.read_csv(\"/content/main/schedule/gtfs23Sept/stops.csv\")\n",
        "stops_df[\"stop_id\"] = [re.sub(\"\\D\", \"\", x.strip()) for x in stops_df[\"stop_id\"]]\n",
        "stops_df[\"stop_id\"] = stops_df[\"stop_id\"].astype(int)\n",
        "all_stops = set([int(stop) for stop in stops_df[\"stop_id\"]])\n",
        "schedules_path = \"/content/main/arrival_time/data/schedules/schedules_line/\"\n",
        "\n",
        "actual_df = pd.read_csv(\"/content/main/arrival_time/data/actual_stops.csv\")\n",
        "size = actual_df.shape[0]\n",
        "actual_df[\"stop_id\"] = [re.sub(\"\\D\", \"\", x.strip()) for x in actual_df[\"stop_id\"]]\n",
        "\n",
        "actual_df[\"succession\"] = actual_df[\"succession\"].astype(str)\n",
        "actual_df[\"numero_lig\"] = actual_df[\"numero_lig\"].astype(str)\n",
        "actual_df[\"variante\"] = actual_df[\"variante\"].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "faulty_stops = [] # Number of unprocessed stops\n",
        "trip_headsign = 0 # index of trip headsign\n",
        "stop_id = 2 # index of stop id\n",
        "dtime = 8\n",
        "n_trips = {}\n",
        "\n",
        "# Save schedule to cache\n",
        "lines = ['29']\n",
        "for line_id in lines:\n",
        "  try:\n",
        "    n_trips_df = pd.read_csv(f\"{schedules_path}schedule_{line_id}_rightTime.csv\")\n",
        "    n_trips_np = n_trips_df.to_numpy()\n",
        "    n_trips[line_id] = n_trips_np\n",
        "\n",
        "  except Exception:\n",
        "    continue"
      ],
      "metadata": {
        "id": "7lWn7adj2tKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in glob(\"/content/main/normalized_lines/*\"):\n",
        "    # print(filename)\n",
        "    base_filename = os.path.basename(filename)\n",
        "    try:\n",
        "        line_id, direction, point_id = [int(v) for v in base_filename.split(\"_\")]\n",
        "        print(line_id, direction, point_id)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    if line != 29:\n",
        "      continue\n",
        "\n",
        "    print(filename)\n",
        "\n",
        "    direction_descr = stops_df.loc[stops_df['stop_id']==direction].iloc[0]['stop_name']\n",
        "\n",
        "    # checking for Technical stops\n",
        "    if (point_id not in all_stops) or (direction not in all_stops):\n",
        "        print('no')\n",
        "        continue\n",
        "\n",
        "    normalized_df = pd.read_csv(filename)\n",
        "    normalized_df = normalized_df[(normalized_df[\"direction_id\"] == direction)]\n",
        "    normalized_df[\"converted_timestamp\"] = normalized_df[\"converted_timestamp\"].apply(lambda x: (pd.Timestamp(x)))\n",
        "\n",
        "    arrival_time = normalized_df['converted_timestamp'].apply(lambda x: (x - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')).to_numpy()\n",
        "\n",
        "    n_trips_np = n_trips[str(line_id)]\n",
        "\n",
        "    start = normalized_df.iloc[0][\"converted_timestamp\"] + pd.Timedelta(hours=2)\n",
        "    end = normalized_df.iloc[-1][\"converted_timestamp\"] + pd.Timedelta(hours=2)\n",
        "    n = n_trips_np[( (n_trips_np[:,trip_headsign] == direction_descr) & (n_trips_np[:,stop_id] == point_id)\n",
        "    & (n_trips_np[:,dtime] >= start) &  (n_trips_np[:,dtime] <= end) )].shape[0]\n",
        "\n",
        "    # Check whether number of cluster is bigger than the data\n",
        "    if arrival_time.shape[0] < n:\n",
        "        faulty_stops.append((line_id, direction, point_id))\n",
        "        continue\n",
        "\n",
        "    # Check whether the schedule is exist or not\n",
        "    if n==0:\n",
        "      faulty_stops.append((line_id, direction, point_id))\n",
        "      continue\n",
        "\n",
        "\n",
        "    '''\n",
        "        kmeans 1d\n",
        "    '''\n",
        "    try:\n",
        "      first = arrival_time[0]\n",
        "      data = np.array([(x-first) for x in arrival_time])\n",
        "      result = ckwrap.ckmeans(data, n)\n",
        "      normalized_df[\"kmeans\"] = np.array(result.labels)\n",
        "    except Exception as e:\n",
        "      faulty_stops.append((line_id, direction, point_id))\n",
        "      continue\n",
        "\n",
        "    # Uncomment this line for doing clustering with other method\n",
        "\n",
        "    #'''\n",
        "    # Kernel Density estimation method\n",
        "    # '''\n",
        "    # first = arrival_time[0]\n",
        "    # a = np.array([(x-first) for x in arrival_time])\n",
        "\n",
        "    # a = np.array([1,1,1,2,3,4,10,12,16,40,45])\n",
        "    # a = a.reshape(-1, 1)\n",
        "\n",
        "    # kde = KernelDensity(kernel='gaussian', bandwidth=0.01).fit(a)\n",
        "\n",
        "    # s = linspace(a[0],a[-1]+2000)\n",
        "    # e = kde.score_samples(s.reshape(-1,1))\n",
        "    # mi, ma = argrelextrema(e, np.less)[0], argrelextrema(e, np.greater)[0]\n",
        "\n",
        "    # clusters = []\n",
        "\n",
        "    # clusters.append(a[a < s[mi][0]])  # most left cluster\n",
        "\n",
        "    # # print all middle cluster\n",
        "    # for i_cluster in range(len(mi)-1):\n",
        "    #     clusters.append(a[(a >= s[mi][i_cluster]) * (a <= s[mi][i_cluster+1])])\n",
        "\n",
        "    # clusters.append(a[a >= s[mi][-1]])  # print most right cluster\n",
        "\n",
        "    # '''\n",
        "    #     dbsscan approach\n",
        "    #     epsilon is 35 seconds\n",
        "    # '''\n",
        "    # dbs = DBSCAN1D(eps=35, min_samples=1)\n",
        "    # labels = dbs.fit_predict(arrival_time)\n",
        "\n",
        "    # '''\n",
        "    #     Jenkins binning algorithm\n",
        "    # '''\n",
        "    # jnb = JenksNaturalBreaks(n)\n",
        "    # jnb.fit(arrival_time)\n",
        "\n",
        "\n",
        "    # normalized_df[\"dbscan\"] = np.array(dbs.labels_)\n",
        "    # normalized_df[\"jenkins\"] = np.array(jnb.labels_)\n",
        "\n",
        "    # kde_labels = []\n",
        "    # for idx, cluster in enumerate(clusters):\n",
        "    #     for x in cluster:\n",
        "    #         kde_labels.append(idx)\n",
        "    # normalized_df[\"KDE\"] = np.array(kde_labels)\n",
        "\n",
        "    save_csv(f'departure_time/cluster_{line_id}_{direction}_{point_id}.csv', normalized_df)"
      ],
      "metadata": {
        "id": "GAE8vQLd22PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing Departure Time"
      ],
      "metadata": {
        "id": "i8WFlcFGux0v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V97LT2U5JrdG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
        "\n",
        "'''\n",
        "Analyze the clustered data\n",
        "'''\n",
        "method = 'kmeans'\n",
        "\n",
        "filename = '/home/sayyor/Desktop/BDMA_Studies/Data Mining/Project/Data/created_data/arrival_time_sample_cluster.csv'\n",
        "normalized_df = pd.read_csv(filename)\n",
        "\n",
        "# only if not CET+1\n",
        "normalized_df.original_timestamp = pd.to_datetime(normalized_df['original_timestamp'])+pd.Timedelta('2h')\n",
        "# df.converted_timestamp = df.converted_timestamp.apply(lambda x: parser.parse(x).timestamp()*int(TO_MS))\n",
        "\n",
        "# getting measures for the clusters for each method\n",
        "evals = {'ch':{},'db':{}, 'ss':{}}\n",
        "normalized_df[\"converted_timestamp\"] = normalized_df[\"converted_timestamp\"].apply(lambda x: (pd.Timestamp(x)))\n",
        "arrival_time = normalized_df['converted_timestamp'].apply(lambda x: (x - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')).to_numpy()\n",
        "arrival_time = arrival_time.reshape(-1, 1)\n",
        "\n",
        "for method in ['dbscan', 'kmeans', 'jenkins', 'KDE']:\n",
        "  # measures[method]={'avr_in_variance':None, 'avr_out_gap':None}\n",
        "  labels = normalized_df[method].to_numpy()\n",
        "  res = calinski_harabasz_score(arrival_time, labels)\n",
        "  evals['ch'][method] = round(res,2)\n",
        "\n",
        "  res = davies_bouldin_score(arrival_time, labels)\n",
        "  evals['db'][method] = round(res,2)\n",
        "\n",
        "  res = silhouette_score(arrival_time, labels)\n",
        "  evals['ss'][method] = round(res,2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.indexing import convert_from_missing_indexer_tuple\n",
        "'''\n",
        "Get Departure Time\n",
        "'''\n",
        "\n",
        "normalized_path = \"/content/main/departure_time/*.csv\"\n",
        "result_path = \"departure_time/results/\"\n",
        "already_done = [os.path.basename(filename) for filename in glob(\"/content/main/departure_time/results/*.csv\")]\n",
        "already_done = [f[:-4] for f in already_done]\n",
        "\n",
        "method = \"kmeans\"\n",
        "\n",
        "for clustered_f in glob(normalized_path):\n",
        "  base_filename = os.path.basename(clustered_f)\n",
        "  (line_id, direction_id, piont_id) = base_filename[8:-4].split(\"_\")\n",
        "\n",
        "  if line_id not in ['29']:\n",
        "    continue\n",
        "\n",
        "  if base_filename[8:-4] in already_done:\n",
        "    continue\n",
        "\n",
        "  print(line_id, direction_id, piont_id)\n",
        "\n",
        "  df = pd.read_csv(clustered_f)\n",
        "  departure_df = pd.DataFrame(columns=['departure_time'])\n",
        "\n",
        "  for cluster in df[method].unique():\n",
        "    records = df.loc[df[method]==cluster]\n",
        "    departure_time = max(list(records.converted_timestamp))\n",
        "    new_row = {\n",
        "        'departure_time':departure_time\n",
        "    }\n",
        "    departure_df = departure_df.append(new_row, ignore_index=True)\n",
        "\n",
        "  result_f = f'{result_path}{line_id}_{direction_id}_{piont_id}.csv'\n",
        "  save_csv(result_f, departure_df)"
      ],
      "metadata": {
        "id": "USl0UH4fuVoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Measuring Regularity\n",
        "'''\n",
        "regularity_faulty = []\n",
        "headways_path = \"/content/main/arrival_time/data/schedules/headways_line/\"\n",
        "for line_id in ['29']:\n",
        "# for line_id in ['1','5', '29', '38', '63', '65', '66', '71', '89']:\n",
        "  print('line',line_id)\n",
        "\n",
        "  headways_file = f'{headways_path}headways_{line_id}_rightTime.csv'\n",
        "  headways_np = np.genfromtxt(headways_file, delimiter=',', dtype=None, encoding=None)\n",
        "  headways_np = headways_np[1:,:]\n",
        "  save_path = \"regularity/results/\"\n",
        "\n",
        "  headways_df = pd.read_csv(headways_file)\n",
        "\n",
        "  stop_id_idx = get_column_idx(\"stop_id\", headways_df)\n",
        "  start_date_idx = get_column_idx(\"start_date\", headways_df)\n",
        "  end_date_idx = get_column_idx(\"end_date\", headways_df)\n",
        "  trip_headsign_idx = get_column_idx(\"trip_headsign\", headways_df)\n",
        "  interv_type_idx = get_column_idx(\"interval_type\", headways_df)\n",
        "  swt_idx = get_column_idx(\"swt\", headways_df)\n",
        "  awt_idx = get_column_idx(\"avg_headway\", headways_df)\n",
        "  init_len = headways_np.shape[1]\n",
        "\n",
        "  start_datetime = [[datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")] for x in headways_np[:,start_date_idx]]\n",
        "  headways_np = np.append(headways_np, start_datetime, axis=1)\n",
        "  end_datetime = [[datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")] for x in headways_np[:,end_date_idx]]\n",
        "  headways_np = np.append(headways_np, end_datetime, axis=1)\n",
        "  start_date_idx = init_len\n",
        "  end_date_idx = init_len+1\n",
        "\n",
        "  ewt_arr = []\n",
        "  ewt_arr.append(['stop_id', 'trip_headsign', 'interval_start', 'interval_end', 'avg_headway', 'swt','awt', 'ewt'])\n",
        "\n",
        "  departure_path = \"/content/main/departure_time/results/\"\n",
        "  departure_files = [f for f in glob(f\"{departure_path}*.csv\") if os.path.basename(f)[:-4].split(\"_\")[0] == line_id]\n",
        "\n",
        "  for real_time in departure_files:\n",
        "    (line_id, direction_id, point_id) = (x for x in os.path.basename(real_time)[:-4].split(\"_\"))\n",
        "    print(line_id, direction_id, point_id)\n",
        "\n",
        "    try:\n",
        "      direction_descr = actual_df.loc[(actual_df['stop_id']==direction_id)].iloc[0]['descr_fr']\n",
        "    except:\n",
        "      regularity_faulty.append((line_id, direction_id, point_id))\n",
        "      continue\n",
        "\n",
        "    # getting in datetime format\n",
        "    real_time_np = np.genfromtxt(real_time, delimiter=',', dtype=None, encoding=None)\n",
        "    real_time_np = real_time_np[1:]\n",
        "    real_time_np = np.char.strip(real_time_np, '\"')\n",
        "    real_time_np = np.array([[datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\")] for x in real_time_np])\n",
        "\n",
        "    # only the periods of the same pointID and directionID and evaluated by regularity\n",
        "    periods = headways_np[( (headways_np[:,stop_id_idx]==point_id) & (headways_np[:, interv_type_idx] == 'regularity') & (headways_np[:, trip_headsign_idx] == direction_descr) )]\n",
        "    print(\"n_periods\", periods.shape[0])\n",
        "\n",
        "    if periods.shape[0]==0:\n",
        "      regularity_faulty.append((line_id, direction_id, point_id))\n",
        "\n",
        "    # getting ewt for each period\n",
        "    for period in periods:\n",
        "      period_start = period[start_date_idx]\n",
        "      period_end = period[end_date_idx]\n",
        "      avg_headway = period[awt_idx]\n",
        "      swt = float(period[swt_idx])\n",
        "\n",
        "      # get departs for that period\n",
        "      period_departs = real_time_np[((real_time_np >= period_start) & (real_time_np <= period_end))]\n",
        "\n",
        "      # sorting by time\n",
        "      period_departs = period_departs[period_departs.argsort()]\n",
        "\n",
        "      # get headway for each pair of departure times\n",
        "      n = period_departs.shape[0]\n",
        "      headways = []\n",
        "      if n == 0:\n",
        "        continue\n",
        "      elif n == 1:\n",
        "        headway = (period_end - period_start).total_seconds()\n",
        "        headway = headway/2\n",
        "        headways.append(headway)\n",
        "      else:\n",
        "        for (first, second) in zip(range(n-1), range(1, n)):\n",
        "            headway = period_departs[second]-period_departs[first]\n",
        "            headways.append(headway)\n",
        "        # convert to seconds each headway\n",
        "        for i in range(len(headways)):\n",
        "            headways[i] = headways[i].total_seconds()\n",
        "\n",
        "      headways = np.array(headways)\n",
        "      # calculate the metrics\n",
        "      square_sum = np.sum(headways**2)\n",
        "      sum_two = 2*np.sum(headways)\n",
        "      awt = square_sum/sum_two\n",
        "      ewt = awt-swt\n",
        "\n",
        "      ewt_arr.append([point_id, direction_descr, period_start, period_end, avg_headway, swt, awt, ewt])\n",
        "\n",
        "  ewt_arr = np.array(ewt_arr)\n",
        "  save_np_csv(save_path+f\"regularity_{line_id}.csv\", ewt_arr)"
      ],
      "metadata": {
        "id": "RgbE0rl9Qtc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine results\n",
        "line_id = '1'\n",
        "path = '/content/main/arrival_time/data/schedules/realFinal/'\n",
        "f = f'{path}schedule_{line_id}_rightTime.csv'\n",
        "df = pd.read_csv(f)\n",
        "n = df.shape[0]\n",
        "\n",
        "lines = ['5', '29', '38', '63', '65', '66', '71', '89']\n",
        "for line_id in lines:\n",
        "  f = f'{path}schedule_{line_id}_rightTime.csv'\n",
        "  df2 = pd.read_csv(f)\n",
        "  n += df2.shape[0]\n",
        "  df = df.append(df2)"
      ],
      "metadata": {
        "id": "BQLYYUF09wTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "remove regularity that is greater than threshold\n",
        "'''\n",
        "threshold = 1000\n",
        "lines = ['1','5', '29', '38', '63', '65', '66', '71', '89']\n",
        "regularity_res_path = '/content/main/regularity/results/'\n",
        "out_path = 'regularity/until1000/'\n",
        "\n",
        "for line_id in lines[1:]:\n",
        "  print(line_id)\n",
        "  regular_f = f'{regularity_res_path}regularity_{line_id}.csv'\n",
        "  out_f = f'{out_path}regularity_{line_id}.csv'\n",
        "  regular_df = pd.read_csv(regular_f)\n",
        "  regular_df = regular_df.loc[(regular_df['ewt'] < 1000) & (regular_df['ewt'] > 0)]\n",
        "  save_csv(out_f, regular_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d82Uu4HlAd5",
        "outputId": "8e9da610-a9b5-489e-b0a5-69959579412f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "29\n",
            "38\n",
            "63\n",
            "65\n",
            "66\n",
            "71\n",
            "89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for line_id in ['1', '5', '38', '63', '65', '66', '71', '89', '29']:\n",
        "  schedules_f = f'{schedules_path}schedule_{line_id}_rightTime.csv'\n",
        "  schedules_df = pd.read_csv(schedules_f)\n",
        "  print(line_id,'punctuality', schedules_df.loc[schedules_df['punctuality_index']!='None'].shape[0])\n",
        "  print(line_id,'regularity', schedules_df.loc[schedules_df['ewt']!='None'].shape[0])\n",
        "  print('\\n')\n",
        "  # schedules_df = schedules_df.drop(['estimated_departure_time', 'punctuality_index', 'avg_headway',\t'swt','awt','ewt'], axis=1, errors='ignore')\n",
        "  # new_f = schedules_f[14:]\n",
        "  # save_csv(new_f, schedules_df)"
      ],
      "metadata": {
        "id": "ltDNpSxHOa5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Transfer awt and ewt to schedules_line files\n",
        "Separate columns awt and ewt\n",
        "Iterate through each line_id for schedules_line and regularity results\n",
        "Each record that has regularity, the same stop_id, the same direction, time within the period\n",
        "  should have the same awt and ewt\n",
        "'''\n",
        "\n",
        "schedules_path = '/content/main/arrival_time/data/schedules/schedules_line/'\n",
        "regularity_res_path = '/content/main/regularity/until1000/'\n",
        "\n",
        "# for line_id in ['1', '5', '29', '38', '63', '65', '66', '71'][1:]:\n",
        "for line_id in ['89']:\n",
        "  print(line_id)\n",
        "\n",
        "  regular_f = f'{regularity_res_path}regularity_{line_id}.csv'\n",
        "  schedules_f = f'{schedules_path}schedule_{line_id}_rightTime.csv'\n",
        "\n",
        "  regular_np = np.genfromtxt(regular_f, delimiter=',', dtype=None, encoding=None)\n",
        "  regular_np = np.char.strip(regular_np, '\"')\n",
        "  regular_np = regular_np[1:,:]\n",
        "  reg_point_id, reg_direction_descr, reg_period_start, reg_period_end, reg_avg_headway, reg_swt, reg_awt, reg_ewt = range(8)\n",
        "\n",
        "  period_start_np = [[datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")] for x in regular_np[:,reg_period_start]]\n",
        "  regular_np = np.append(regular_np, period_start_np, axis=1)\n",
        "  reg_period_start = 8\n",
        "  period_end_np = [[datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")] for x in regular_np[:,reg_period_end]]\n",
        "  regular_np = np.append(regular_np, period_end_np, axis=1)\n",
        "  reg_period_end = 9\n",
        "\n",
        "  schedules_np = np.genfromtxt(schedules_f, delimiter=',', dtype=None, encoding=None)\n",
        "  schedules_np = np.char.strip(schedules_np, '\"')\n",
        "  schedules_np = schedules_np[1:,:]\n",
        "  sch_direction_descr, _, sch_point_id, _, _, interv_type, _, _, sch_depart_time = range(9)\n",
        "\n",
        "  # setting empty columns for avg_headway, swt, awt and ewt\n",
        "  schedules_np = np.append(schedules_np,  np.full((schedules_np.shape[0],1), None), axis=1)\n",
        "  schedules_np = np.append(schedules_np,  np.full((schedules_np.shape[0],1), None), axis=1)\n",
        "  schedules_np = np.append(schedules_np,  np.full((schedules_np.shape[0],1), None), axis=1)\n",
        "  schedules_np = np.append(schedules_np,  np.full((schedules_np.shape[0],1), None), axis=1)\n",
        "  sch_avg_headway = 9\n",
        "  sch_swt = 10\n",
        "  sch_awt = 11\n",
        "  sch_ewt = 12\n",
        "\n",
        "  sch_depart_time_np = [[datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")] for x in schedules_np[:,sch_depart_time]]\n",
        "  schedules_np = np.append(schedules_np, sch_depart_time_np, axis=1)\n",
        "  sch_depart_time = 13\n",
        "\n",
        "  i = 0\n",
        "  for period in regular_np:\n",
        "    i+=1\n",
        "    if i % 100 == 0:\n",
        "      print(i,regular_np.shape[0])\n",
        "    match_indeces = np.where((\n",
        "        (schedules_np[:,interv_type] == 'regularity') &\n",
        "        (schedules_np[:,sch_point_id] == period[reg_point_id]) &\n",
        "        (schedules_np[:,sch_direction_descr] == period[reg_direction_descr]) &\n",
        "        (schedules_np[:,sch_depart_time] >= period[reg_period_start]) &\n",
        "        (schedules_np[:,sch_depart_time] <= period[reg_period_end])\n",
        "        ))\n",
        "\n",
        "    for index in match_indeces[0]:\n",
        "      schedules_np[index, sch_avg_headway] = period[reg_avg_headway]\n",
        "      schedules_np[index, sch_swt] = period[reg_swt]\n",
        "      schedules_np[index, sch_awt] = period[reg_awt]\n",
        "      schedules_np[index, sch_ewt] = period[reg_ewt]\n",
        "\n",
        "  schedules_np = np.delete(schedules_np, sch_depart_time, axis=1)\n",
        "\n",
        "  columns = np.array([['trip_headsign', 'direction_id', 'stop_id', 'departure_time', 'headway', 'class', 'date', 'line', 'departure_date', 'avg_headway', 'swt', 'awt', 'ewt']])\n",
        "  schedules_np = np.append(columns, schedules_np, axis=0)\n",
        "  save_np_csv(schedules_f[14:], schedules_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ArfKzcEWy0X",
        "outputId": "dbcecea3-5366-4d2b-e546-4582b270e629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89\n",
            "100 314\n",
            "200 314\n",
            "300 314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Transfer estimated_departure_time and punctuality index to schedules_line files\n",
        "Separate columns estimated_departure_time and punctuality_index\n",
        "Iterate through each line_id for schedules_line and all files of the same line in punctuality/results\n",
        "Each record that has punctuality, the same stop_id (in the filename),\n",
        "  the same trip_headsign, the same scheduled_time\n",
        "  should have the same estimated_departure_time and punctuality_index\n",
        "'''\n",
        "\n",
        "stops_df = pd.read_csv(\"/content/main/schedule/gtfs23Sept/stops.csv\")\n",
        "stops_df[\"stop_id\"] = [re.sub(\"\\D\", \"\", x.strip()) for x in stops_df[\"stop_id\"]]\n",
        "stops_df[\"stop_id\"] = stops_df[\"stop_id\"].astype(int)\n",
        "\n",
        "schedules_path = '/content/main/arrival_time/data/schedules/schedules_line/'\n",
        "regularity_res_path = '/content/main/punctuality/result/'\n",
        "\n",
        "# for line_id in ['1', '5', '29', '38', '63', '65', '66', '89', '71'][1:]:\n",
        "for line_id in ['89']:\n",
        "  print(line_id)\n",
        "\n",
        "  # schedules_f = f'{schedules_path}schedule_{line_id}_rightTime.csv'\n",
        "  schedules_f = '/content/main/arrival_time/data/schedules/schedules_line/schedule_89_rightTime.csv'\n",
        "\n",
        "  schedules_np = np.genfromtxt(schedules_f, delimiter=',', dtype=None, encoding=None)\n",
        "  schedules_np = np.char.strip(schedules_np, '\"')\n",
        "  schedules_np = schedules_np[1:,:]\n",
        "  sch_direction_descr, _, sch_point_id, _, _, interv_type, _, _, sch_depart_time, _,_,_,_ = range(13)\n",
        "\n",
        "  # setting empty columns for avg_headway, swt, awt and ewt\n",
        "  schedules_np = np.append(schedules_np,  np.full((schedules_np.shape[0],1), None), axis=1)\n",
        "  schedules_np = np.append(schedules_np,  np.full((schedules_np.shape[0],1), None), axis=1)\n",
        "  sch_est_depart_t = 13\n",
        "  sch_punct_idx = 14\n",
        "\n",
        "  sch_depart_time_np = [[pd.to_datetime(x)] for x in schedules_np[:,sch_depart_time]]\n",
        "  schedules_np = np.append(schedules_np, sch_depart_time_np, axis=1)\n",
        "  sch_depart_time = 15\n",
        "\n",
        "  regular_fs = [f for f in glob(f\"{regularity_res_path}*.csv\") if os.path.basename(f).split(\"_\")[0] == line_id]\n",
        "\n",
        "  for regular_f in regular_fs:\n",
        "    (_, direction_id, point_id, _) = os.path.basename(regular_f)[:-4].split(\"_\")\n",
        "    direction_descr = stops_df.loc[stops_df['stop_id']==int(direction_id)].iloc[0]['stop_name']\n",
        "    print('f',regular_f)\n",
        "\n",
        "    regular_np = np.genfromtxt(regular_f, delimiter=',', dtype=None, encoding=None)\n",
        "    regular_np = np.char.strip(regular_np, '\"')\n",
        "    try:\n",
        "      regular_np = regular_np[1:,:]\n",
        "    except:\n",
        "      continue\n",
        "    reg_headsign, _, _, reg_sch_time, reg_punct_idx, reg_est_d_t, _, _ = range(8)\n",
        "    # reg_headsign, _, _, _, reg_sch_time, reg_punct_idx, reg_est_d_t, _, _ = range(9)\n",
        "\n",
        "    reg_sch_time_np = [[pd.to_datetime(x)] for x in regular_np[:,reg_sch_time]]\n",
        "    regular_np = np.append(regular_np, reg_sch_time_np, axis=1)\n",
        "    reg_sch_time = 8\n",
        "    # reg_sch_time=9\n",
        "\n",
        "    match_indeces = np.where((\n",
        "      (schedules_np[:,interv_type] == 'punctuality') &\n",
        "      (schedules_np[:,sch_point_id] == point_id) &\n",
        "      (schedules_np[:,sch_direction_descr] == direction_descr)\n",
        "      ))\n",
        "\n",
        "    i = 0\n",
        "    for period in regular_np:\n",
        "      i+=1\n",
        "      if i % 100 == 0:\n",
        "        print(i,regular_np.shape[0])\n",
        "\n",
        "      for m_idx in match_indeces[0]:\n",
        "        if schedules_np[m_idx, sch_depart_time] == period[reg_sch_time]:\n",
        "          schedules_np[m_idx, sch_est_depart_t] = period[reg_est_d_t]\n",
        "          schedules_np[m_idx, sch_punct_idx] = period[reg_punct_idx]\n",
        "\n",
        "  schedules_np = np.delete(schedules_np, sch_depart_time, axis=1)\n",
        "  columns = np.array([['trip_headsign', 'direction_id', 'stop_id', 'departure_time', 'headway', 'class', 'date', 'line', 'departure_date','avg_headway', 'swt', 'awt', 'ewt', 'estimated_departure_time', 'punctuality_index']])\n",
        "  schedules_np = np.append(columns, schedules_np, axis=0)\n",
        "  save_np_csv(schedules_f[14:], schedules_np)"
      ],
      "metadata": {
        "id": "fNrIJff33md9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Converting headway schedules to real time (25-->1 of the next day)'''\n",
        "headways_path = \"/content/main/arrival_time/data/schedules/headways_line/*.csv\"\n",
        "\n",
        "def get_column_idx(column_label, df):\n",
        "  return [idx for idx, col in enumerate(df.columns) if col==column_label][0]\n",
        "\n",
        "def get_real_date(year, month, day, hour, minute, second):\n",
        "  date = datetime.datetime(year,month,day)\n",
        "  if hour > 23:\n",
        "    date += datetime.timedelta(days=1)\n",
        "    hour = hour % 24\n",
        "  date += datetime.timedelta(hours=hour, minutes=minute, seconds=second)\n",
        "  return date\n",
        "\n",
        "for headways_file in glob(headways_path):\n",
        "  print(headways_file)\n",
        "\n",
        "  headways_df = pd.read_csv(headways_file)\n",
        "  headways_np = headways_df.to_numpy()\n",
        "  init_len = headways_np.shape[1]\n",
        "\n",
        "  start_t_idx = get_column_idx(\"interval_start\", headways_df)\n",
        "  end_t_idx = get_column_idx(\"interval_end\", headways_df)\n",
        "  date_idx = get_column_idx(\"date\", headways_df)\n",
        "\n",
        "  # splitting times into their components\n",
        "  time_split = np.array([[int(x) for x in t.split(\":\")] for t in headways_np[:,start_t_idx]])\n",
        "  headways_np = np.append(headways_np, time_split, axis=1)\n",
        "  start_h_idx = init_len\n",
        "  start_m_idx = init_len+1\n",
        "  start_s_idx = init_len+2\n",
        "\n",
        "  time_split = np.array([[int(x) for x in t.split(\":\")] for t in headways_np[:,end_t_idx]])\n",
        "  headways_np = np.append(headways_np, time_split, axis=1)\n",
        "  end_h_idx = init_len+3\n",
        "  end_m_idx = init_len+4\n",
        "  end_s_idx = init_len+5\n",
        "\n",
        "  # splitting the date into its components\n",
        "  headways_np[:, date_idx] = headways_np[:, date_idx].astype(str)\n",
        "  date_split = np.array([[int(d[:4]), int(d[4:6]), int(d[6:])] for d in headways_np[:,date_idx]])\n",
        "  date_y_idx = init_len+6\n",
        "  date_m_idx = init_len+7\n",
        "  date_d_idx = init_len+8\n",
        "  headways_np = np.append(headways_np, date_split, axis=1)\n",
        "\n",
        "  start_date = np.array([ [get_real_date(y,mo,d,h,m,s)] for [y,mo,d,h,m,s] in headways_np[:,[date_y_idx, date_m_idx, date_d_idx, start_h_idx, start_m_idx, start_s_idx]] ])\n",
        "  end_date = np.array([ [get_real_date(y,mo,d,h,m,s)] for [y,mo,d,h,m,s] in headways_np[:,[date_y_idx, date_m_idx, date_d_idx, end_h_idx, end_m_idx, end_s_idx]] ])\n",
        "\n",
        "  headways_np = np.append(headways_np, start_date, axis=1)\n",
        "  headways_np = np.append(headways_np, end_date, axis=1)\n",
        "\n",
        "  to_delete = [start_h_idx, start_m_idx, start_s_idx, end_h_idx, end_m_idx, end_s_idx, date_y_idx, date_m_idx, date_d_idx]\n",
        "  headways_np = np.delete(headways_np, to_delete, 1)\n",
        "\n",
        "  # adding the column to the beginning\n",
        "  headers = list(headways_df.columns)\n",
        "  headers.extend(['start_date', 'end_date'])\n",
        "  headers = np.array([headers])\n",
        "  headways_np = np.concatenate((headers, headways_np))\n",
        "\n",
        "  save_np_csv(headways_file[14:-4]+'_rightTime.csv', headways_np)"
      ],
      "metadata": {
        "id": "8CbplEbUWBsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Converting schedules to real time (25-->1 of the next day)'''\n",
        "headways_path = \"/content/main/arrival_time/data/schedules/schedules_line/*.csv\"\n",
        "\n",
        "for headways_file in glob(headways_path):\n",
        "  print(headways_file)\n",
        "  date_idx = get_column_idx(\"date\", headways_df)\n",
        "  depart_t = get_column_idx(\"departure_time\", headways_df)\n",
        "\n",
        "  headways_df = pd.read_csv(headways_file)\n",
        "  headways_np = headways_df.to_numpy()\n",
        "  init_len = headways_np.shape[1]\n",
        "\n",
        "  # splitting times into their components\n",
        "  time_split = np.array([[int(x) for x in t.split(\":\")] for t in headways_np[:,depart_t]])\n",
        "  headways_np = np.append(headways_np, time_split, axis=1)\n",
        "  start_h_idx = init_len\n",
        "  start_m_idx = init_len+1\n",
        "  start_s_idx = init_len+2\n",
        "\n",
        "  # splitting the date into its components\n",
        "  headways_np[:, date_idx] = headways_np[:, date_idx].astype(str)\n",
        "  date_split = np.array([[int(d[:4]), int(d[4:6]), int(d[6:])] for d in headways_np[:,date_idx]])\n",
        "  date_y_idx = init_len+3\n",
        "  date_m_idx = init_len+4\n",
        "  date_d_idx = init_len+5\n",
        "  headways_np = np.append(headways_np, date_split, axis=1)\n",
        "\n",
        "  start_date = np.array([ [get_real_date(y,mo,d,h,m,s)] for [y,mo,d,h,m,s] in headways_np[:,[date_y_idx, date_m_idx, date_d_idx, start_h_idx, start_m_idx, start_s_idx]] ])\n",
        "\n",
        "  headways_np = np.append(headways_np, start_date, axis=1)\n",
        "\n",
        "  to_delete = [start_h_idx, start_m_idx, start_s_idx, date_y_idx, date_m_idx, date_d_idx]\n",
        "  headways_np = np.delete(headways_np, to_delete, 1)\n",
        "\n",
        "  # adding the column to the beginning\n",
        "  headers = list(headways_df.columns)\n",
        "  headers.extend(['departure_date'])\n",
        "  headers = np.array([headers])\n",
        "  headways_np = np.concatenate((headers, headways_np))\n",
        "\n",
        "  save_np_csv(headways_file[14:-4]+'_rightTime.csv', headways_np)\n",
        "# # periods = headways_np[( (headways_np[:,stop_id_idx]==point_id) & (headways_np[:, interv_type] == 'regularity') )]"
      ],
      "metadata": {
        "id": "Acxni6Me8Tv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faulty_stops = [] # Number of unprocessed stops\n",
        "trip_headsign = 0 # index of trip headsign\n",
        "stop_id = 2 # index of stop id\n",
        "dtime = 8\n",
        "n_trips = {}\n",
        "\n",
        "# Save schedule to cache\n",
        "lines = ['1', '5', '2', '38', '63', '65', '66', '71', '89']\n",
        "for line_id in lines:\n",
        "  try:\n",
        "    n_trips_df = pd.read_csv(f\"{schedules_path}schedule_{line_id}_rightTime.csv\")\n",
        "    n_trips_np = n_trips_df.to_numpy()\n",
        "    n_trips[line_id] = n_trips_np\n",
        "\n",
        "  except Exception:\n",
        "    continue"
      ],
      "metadata": {
        "id": "F-DpWyh6mkCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing Punctuality"
      ],
      "metadata": {
        "id": "HZUB_b39VbqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index definition\n",
        "stop_type = 5\n",
        "index = 10\n",
        "punc_index = 9\n",
        "sched_dtime = 8\n",
        "real_dtime = 0\n",
        "real_idx = 2\n",
        "marked = 1\n",
        "est_dtime = 11\n",
        "est_dtime_marked = 12\n",
        "punc_index_marked = 13\n",
        "headway_idx = 6\n",
        "stop = 2\n",
        "\n",
        "for filename in glob(\"/content/main/departure_time/results/*.csv\"):\n",
        "  file_info = filename.split(\"/\")[5].split(\"_\")\n",
        "  line_id, direction_id, stop_id = file_info[0], file_info[1], re.sub(\"\\D\", \"\", file_info[2].strip())\n",
        "\n",
        "  if \"29\" != line_id:\n",
        "    continue\n",
        "\n",
        "  real_data = pd.read_csv(filename)\n",
        "  start = pd.to_datetime(real_data.iloc[0]['departure_time']) + pd.Timedelta(hours=2)\n",
        "  end = pd.to_datetime(real_data.iloc[-1]['departure_time']) + pd.Timedelta(hours=2)\n",
        "\n",
        "  direction_descr = stops_df[stops_df['stop_id'] == int(direction_id)].iloc[0]['stop_name']\n",
        "\n",
        "  n_trips_np = n_trips[line_id]\n",
        "  n_trips_np[:,sched_dtime] = pd.to_datetime(n_trips_np[:,sched_dtime])\n",
        "  n_trips_np = n_trips_np[(n_trips_np[:,trip_headsign] == direction_descr) & (n_trips_np[:,stop] == int(stop_id)) & (n_trips_np[:, sched_dtime] >= start) & (n_trips_np[:, sched_dtime] <= end)]\n",
        "\n",
        "  # Add initial punctuality and indexes for processing\n",
        "  est_dtime_arr = [None for i in range(n_trips_np.shape[0])]\n",
        "  est_dtime_marked_arr = [None for i in range(n_trips_np.shape[0])]\n",
        "  punc_idx_marked_arr = [None for i in range(n_trips_np.shape[0])]\n",
        "  punctuality = [None for i in range(n_trips_np.shape[0])]\n",
        "  idxs = [i for i in range(n_trips_np.shape[0])]\n",
        "  n_trips_np = np.c_[n_trips_np, punctuality, idxs, est_dtime_arr, est_dtime_marked_arr, punc_idx_marked_arr]\n",
        "\n",
        "  # Add variables to real data for punctuality processing\n",
        "  real_data['departure_time'] = real_data['departure_time'].apply(lambda x: pd.to_datetime(x) + pd.Timedelta(hours=2))\n",
        "  real_data['marked'] = [False for i in range(real_data.shape[0])]\n",
        "  real_data['index'] = [i for i in range(real_data.shape[0])]\n",
        "  real_data = real_data.to_numpy()\n",
        "\n",
        "  # Filter punctuality only trips\n",
        "  trips = n_trips_np[n_trips_np[:, stop_type] == 'punctuality']\n",
        "\n",
        "  for idx, trip in enumerate(trips):\n",
        "    time = trip[sched_dtime] - pd.Timedelta(minutes=1)\n",
        "    flt_rt = real_data[real_data[:,real_dtime] >= time]\n",
        "    headway = (n_trips_np[idx+1][headway_idx]) if idx+1 < len(n_trips_np) - 1 else 1\n",
        "    punc_idx = 'NaN'\n",
        "\n",
        "    for rt in flt_rt:\n",
        "      ori_idx = rt[real_idx]\n",
        "      punc_idx = (rt[real_dtime] - time).seconds / headway\n",
        "      n_trips_np[trip[index]][est_dtime] = rt[real_dtime]\n",
        "      break\n",
        "    n_trips_np[trip[index]][punc_index] = punc_idx\n",
        "\n",
        "    punc_idx_marked = \"NaN\"\n",
        "    for rt in flt_rt:\n",
        "      ori_idx = rt[real_idx]\n",
        "      if real_data[ori_idx][marked]:\n",
        "        continue\n",
        "      punc_idx_marked = (rt[real_dtime] - time).seconds / headway\n",
        "      real_data[ori_idx][marked] = True\n",
        "      n_trips_np[trip[index]][est_dtime_marked] = rt[real_dtime]\n",
        "      break\n",
        "    n_trips_np[trip[index]][punc_index_marked] = punc_idx_marked\n",
        "\n",
        "  schedule = pd.DataFrame(n_trips_np)\n",
        "  schedule = schedule[schedule[stop_type] == 'punctuality'].drop(columns=[1,2,6,7,index])\n",
        "  save_csv(f\"punctuality/result/{line_id}_{direction_id}_{stop_id}_punctuality.csv\", schedule)\n"
      ],
      "metadata": {
        "id": "e7SXrWjsVd7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctualities = {}\n",
        "for filename in glob(\"/content/main/punctuality/result/*.csv\"):\n",
        "    file_info = filename.split(\"/\")[5].split(\"_\")\n",
        "    line_id, direction_id, stop_id = file_info[0], file_info[1], file_info[2]\n",
        "    punctuality = punctualities.setdefault(line_id, None)\n",
        "\n",
        "    punctuality_df = pd.read_csv(filename)\n",
        "    punctuality_df[\"stop_id\"] = stop_id\n",
        "\n",
        "    if punctuality is None:\n",
        "      punctuality = punctuality_df\n",
        "    else:\n",
        "      punctuality = pd.concat([punctuality, punctuality_df], ignore_index=True)\n",
        "\n",
        "    punctualities[line_id] = punctuality\n",
        "    print(file_info)\n",
        "\n",
        "for line_id, df in punctualities.items():\n",
        "  save_csv(f\"punctuality/combined/{line_id}_punctuality.csv\", df)"
      ],
      "metadata": {
        "id": "e9QdMLLUd9g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv = \"/content/main/arrival_time/data/schedules/schedules_line/schedule_71_rightTime_update.csv\"\n",
        "combined_df = pd.read_csv().to_numpy()\n",
        "# for filename in glob(\"/content/main/punctuality/combined/*.csv\"):\n",
        "punctuality_df = pd.read_csv(\"/content/main/punctuality/combined/*.csv\").to_numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "g_ClLLFxUWez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "cEUJzjVCg7I_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}