{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+Pe8JZGFUOnI/qna/p2oo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lorenc1o/DataMining_HackMyRide/blob/main/sanitize_data_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8jF-sA3ENEx",
        "outputId": "04587066-6ab0-4852-c7f2-86ae45d846ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2426  100  2426    0     0  78258      0 --:--:-- --:--:-- --:--:-- 80866\n",
            "OK\n",
            "18 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 13.3 MB of archives.\n",
            "After this operation, 30.7 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 124013 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.41.9_amd64.deb ...\n",
            "Unpacking gcsfuse (0.41.9) ...\n",
            "Setting up gcsfuse (0.41.9) ...\n",
            "2022/12/07 21:09:54.879915 Start gcsfuse/0.41.9 (Go version go1.18.4) for app \"\" using mount point: /content/main\n",
            "2022/12/07 21:09:54.896835 Opening GCS connection...\n",
            "2022/12/07 21:09:56.078814 Mounting file system \"datamining-ulb\"...\n",
            "2022/12/07 21:09:56.079508 File system has been successfully mounted.\n",
            "data\t    Lines_vehiclePositions  schedule\n",
            "esri_files  ordered_lines\t    vehiclePositions\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "\n",
        "!mkdir main\n",
        "!gcsfuse --implicit-dirs datamining-ulb main\n",
        "\n",
        "!ls /content/main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime\n",
        "import decimal\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "decimal.getcontext().prec = 3\n",
        "TO_MS = decimal.Decimal(\"1000\")\n",
        "\n",
        "# Helper function\n",
        "def epoch_ms_to_datetime(epoch_ms: int) -> datetime.datetime:\n",
        "    return datetime.datetime.fromtimestamp(epoch_ms / int(TO_MS))"
      ],
      "metadata": {
        "id": "kdDfGg2ZGQp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract actual stops\n",
        "actual_df = pd.read_csv(\"/content/main/data/actual_stops.csv\")\n",
        "size = actual_df.shape[0]\n",
        "actual_df[\"stop_id\"] = [re.sub(\"\\D\", \"\", x.strip()) for x in actual_df[\"stop_id\"]]\n",
        "\n",
        "actual_df[\"succession\"] = actual_df[\"succession\"].astype(str)\n",
        "actual_df[\"numero_lig\"] = actual_df[\"numero_lig\"].astype(str)\n",
        "actual_df[\"variante\"] = actual_df[\"variante\"].astype(str)\n",
        "actual_stops = set(actual_df[\"stop_id\"])\n",
        "\n",
        "# Extract Line stop sequence\n",
        "extracted_lines = {}\n",
        "\n",
        "delimiters = np.array([\"_\" for _ in range(size)])\n",
        "stops_info = (actual_df[\"numero_lig\"].to_numpy() + delimiters +  actual_df[\"succession\"].to_numpy() + delimiters + actual_df[\"variante\"].to_numpy() + delimiters + actual_df[\"stop_id\"].to_numpy()).tolist()\n",
        "\n",
        "prev_succession = -999999\n",
        "direction = \"\"\n",
        "for stops in stops_info[::-1]:\n",
        "    info = stops.split(\"_\")\n",
        "    if int(prev_succession) < int(info[1]):\n",
        "        direction = info[3]\n",
        "    \n",
        "    data = extracted_lines.setdefault(info[0], {})\n",
        "    \n",
        "    succession_info = data.setdefault(info[3], {})\n",
        "    succession_info[direction] = info[1]\n",
        "    \n",
        "    data[info[3]] = succession_info\n",
        "    extracted_lines[info[0]] = data\n",
        "    \n",
        "    line_direction = data.setdefault(\"direction\", {})\n",
        "    visited_stops = line_direction.setdefault(direction, set())\n",
        "    visited_stops.add(info[3])\n",
        "    \n",
        "    line_direction[direction] = visited_stops\n",
        "    data[\"direction\"] = line_direction\n",
        "    \n",
        "    prev_succession = info[1]\n",
        "\n",
        "'''\n",
        "Pair stops with the succession line, \n",
        " 1. if there's multiple sucession line check whether direction is the same\n",
        " 2. If not, try to find direction id in the direction stops\n",
        " 3. otherwise set sucession to 99999\n",
        "'''\n",
        "\n",
        "lines = []\n",
        "names = set()\n",
        "for file in glob(\"/content/main/ordered_lines/*.csv\"):\n",
        "  name = file.strip().split(\"/\")[4].split(\"_\")[0]\n",
        "  names.add(name)\n",
        "\n",
        "print(names)\n",
        "for file in glob(\"/content/main/Lines_vehiclePositions/*.csv\"):\n",
        "    if \"null\" in file or file in names:\n",
        "      continue\n",
        "\n",
        "    df = pd.read_csv(file)\n",
        "    df[\"lineID\"] = df[\"lineID\"].astype(str)\n",
        "    df[\"directionID\"] = df[\"directionID\"].astype(str)\n",
        "    df[\"pointID\"] = df[\"pointID\"].astype(str).apply(lambda x: x.strip().split(\" \")[0])\n",
        "    \n",
        "    delimiters = np.array([\"_\" for _ in range(df.shape[0])])\n",
        "    stops_infos = (df[\"lineID\"].to_numpy() + delimiters + df[\"directionID\"].to_numpy() + delimiters + df[\"pointID\"].to_numpy() + delimiters + df.index.astype(str).values).tolist()\n",
        "    \n",
        "    succession_arr = []\n",
        "    for stop in stops_infos:\n",
        "        info = stop.split(\"_\")\n",
        "        data = extracted_lines[info[0]]\n",
        "        \n",
        "        succession = 999999\n",
        "        if info[2] not in data:\n",
        "            succession_arr.append(succession)\n",
        "            continue\n",
        "        \n",
        "        succession_info = data[info[2]]\n",
        "        if info[1] not in succession_info:\n",
        "            succession_arr.append(succession)\n",
        "            continue\n",
        "        \n",
        "        succession = succession_info[info[1]]\n",
        "        succession_arr.append(succession)\n",
        "    \n",
        "    df[\"succession\"] = np.array(succession_arr)\n",
        "    \n",
        "    df[\"lineID\"] = df[\"lineID\"].astype(int)\n",
        "    df[\"directionID\"] = df[\"directionID\"].astype(int)\n",
        "    df[\"pointID\"] = df[\"pointID\"].astype(int)\n",
        "    df[\"succession\"] = df[\"succession\"].astype(int)\n",
        "\n",
        "    name = file.strip().split(\"/\")[4].split(\"_\")[0]\n",
        "    df.sort_values([\"directionID\", \"time\",  \"succession\",], ascending=[True, True, True]).to_csv(f\"/content/main/ordered_lines/{name}_ordered.csv\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2plV2dAvGjIZ",
        "outputId": "19762878-6fef-464a-a84a-442c303afc24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Line17', 'Line27', 'Line3', 'Line36', 'Line48', 'Line6', 'Line12', 'Line92', 'Line61', 'Line82', 'Line76', 'Line70', 'Line93', 'Line95', 'Line29', 'Line39', 'Line1', 'Line21', 'Line51', 'Line56', 'Line88', 'Line33', 'Line64', 'Line80', 'Line62', 'Line28', 'Line25', 'Line54', 'Line98', 'Line49', 'Line41', 'Line38', 'Line20', 'Line14', 'Line19', 'Line2', 'Line65', 'Line78', 'Line81', 'Line71', 'Line47', 'Line69', 'Line57', 'Line55', 'Line63', 'Line34', 'Line7', 'Line42', 'Line74', 'Line86', 'Line59', 'Line75', 'Line89', 'Line50', 'Line79', 'Line87', 'Line53', 'Line46', 'Line5', 'Line4', 'Line8', 'Line97', 'Line58', 'Line66', 'Line43', 'Line13', 'Line72', 'Line83', 'Line37', 'Line44', 'Line60', 'Line45', 'Line77', 'Line9'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Normalize the distance\n",
        " The algorithm will work as follow:\n",
        " 1. We will change the distance for non-zero distance point by subtracting the time with how much time passed since the bus in the 0 distance of the point\n",
        " 2. We will use the bus/tram average speed to calculate the distance passed\n",
        "'''\n",
        "\n",
        "AVERAGE_SPEED = decimal.Decimal(\"4.667\") # in metre/second\n",
        "\n",
        "# TODO: Changes it for every line\n",
        "line_df = pd.read_csv(\"line_71_sucession.csv\")\n",
        "\n",
        "# Remove outlier\n",
        "line_df = line_df[line_df[\"succession\"] != 999999]\n",
        "\n",
        "columns = [\"time\", \"lineID\", \"directionID\", \"distancefromPoint\", \"pointID\", \"succession\"]\n",
        "\n",
        "size = line_df.shape[0]\n",
        "delimiters = np.array([\"_\" for _ in range(size)])\n",
        "\n",
        "line_infos = None\n",
        "for idx, column in enumerate(columns):\n",
        "    line_df[column] = line_df[column].astype(str)\n",
        "    if line_infos is None:\n",
        "        line_infos = line_df[column].to_numpy() + delimiters\n",
        "    elif line_infos is not None and idx < len(columns)-1:\n",
        "        line_infos += line_df[column].to_numpy() + delimiters\n",
        "    else:\n",
        "        line_infos += line_df[column].to_numpy()\n",
        "\n",
        "normalized_timestamp = []\n",
        "normalized_distance = []\n",
        "for line_info in line_infos:\n",
        "    line = line_info.split(\"_\")\n",
        "\n",
        "    # Define var\n",
        "    timestamp = int(line[0])\n",
        "    distance = decimal.Decimal(line[3])\n",
        "    \n",
        "    if distance == 0:\n",
        "        normalized_timestamp.append(epoch_ms_to_datetime(timestamp))\n",
        "        normalized_distance.append(distance)\n",
        "        continue\n",
        "        \n",
        "    # Normalized the timestamp\n",
        "    time_passed = distance / AVERAGE_SPEED\n",
        "    timestamp -= (int(time_passed*TO_MS))\n",
        "    \n",
        "    normalized_timestamp.append(epoch_ms_to_datetime(timestamp))\n",
        "    normalized_distance.append(math.floor(abs(distance - (AVERAGE_SPEED*time_passed))))\n",
        "\n",
        "\n",
        "line_df[\"normalized_timestamp\"] = np.array(normalized_timestamp)\n",
        "line_df[\"normalized_distance\"] = np.array(normalized_distance)\n",
        "\n",
        "for idx, column in enumerate(columns):\n",
        "    line_df[column] = line_df[column].astype(int)\n",
        "\n",
        "\n",
        "line_df[\"time\"] = line_df[\"time\"].apply(lambda x: epoch_ms_to_datetime(x))\n",
        "line_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "line_df.sort_values([\"directionID\", \"succession\", \"normalized_timestamp\"], ascending=[True, True, True]).to_csv(\"line_71_normalized.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85KgMY27UR3A",
        "outputId": "213a653e-2ea6-4c5c-a4da-9a76e49e9f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    }
  ]
}